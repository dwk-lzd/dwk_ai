# ollama

它是一个让你能通过简单命令在本地轻松下载、运行和管理大语言模型的工具，支持
GPU加速和类OPENAI 接口，适合本地部署和开发

Meta Llama 羊驼
deepseek-r1:1.5b 参数的尺寸
Qwen 
ollama pull deepseek-r1:1.5b 下载模型
ollama run deepseek-r1:1.5b 运行模型

在11434端口提供API 调用